# ğŸš€ Reconnaissance d'EntitÃ©s NommÃ©es avec Multiples ModÃ¨les et Interface Web ğŸš€

Ce projet a Ã©tÃ© dÃ©veloppÃ© avec passion et dÃ©vouement dans le cadre des cours "Interfaces web pour le TAL" et "RÃ©seaux de neurones", brillamment dirigÃ©s par M. LoÃ¯c Grobol Ã  l'UniversitÃ© Paris Nanterre.



## ğŸ“š Introduction

Dans le cadre de notre exploration approfondie de l'univers de la reconnaissance d'entitÃ©s nommÃ©es (NER), nous avons mis en Å“uvre une approche mÃ©thodique pour Ã©tudier et comparer l'efficacitÃ© de divers modÃ¨les de renom dans ce domaine. Notre quÃªte nous a menÃ©s Ã  sÃ©lectionner et Ã  utiliser trois modÃ¨les prÃ©-entraÃ®nÃ©s particuliÃ¨rement distinguÃ©s, chacun apportant une contribution unique Ã  notre comprÃ©hension globale de la NER. Voici les modÃ¨les que nous avons choisis pour notre voyage :

- **spaCy** : Reconnu pour sa rapiditÃ© et son efficacitÃ©, le modÃ¨le fr_core_news_sm de spaCy offre une base solide pour la reconnaissance d'entitÃ©s en franÃ§ais. C'est un choix privilÃ©giÃ© pour les applications nÃ©cessitant une grande vitesse de traitement et une bonne prÃ©cision.

- **dslim/bert-base-NER** : Ce modÃ¨le, basÃ© sur l'architecture Transformer de BERT et prÃ©-entraÃ®nÃ© sur un vaste corpus, est spÃ©cialisÃ© dans la reconnaissance d'entitÃ©s nommÃ©es. Il est connu pour sa capacitÃ© Ã  comprendre le contexte des mots dans des phrases, ce qui le rend extrÃªmement efficace pour identifier avec prÃ©cision divers types d'entitÃ©s.

- **Babelscape/wikineural-multilingual-ner** : Ce modÃ¨le tire parti de l'apprentissage profond pour offrir une reconnaissance d'entitÃ©s nommÃ©es dans plusieurs langues, y compris le franÃ§ais. GrÃ¢ce Ã  son entraÃ®nement sur des donnÃ©es provenant de Wikipedia, il est capable de reconnaÃ®tre un large Ã©ventail d'entitÃ©s Ã  travers diffÃ©rents domaines.

- **Fine-tuned TinyBERT** : Au-delÃ  de l'adoption de modÃ¨les prÃ©-entraÃ®nÃ©s rÃ©putÃ©s, nous avons Ã©galement pris l'initiative ambitieuse d'adapter un modÃ¨le pour rÃ©pondre prÃ©cisÃ©ment Ã  nos exigences particuliÃ¨res. En tirant parti du [MultiNERD dataset](https://huggingface.co/datasets/Babelscape/multinerd) , Nous avons procÃ©dÃ© au fine-tuning d'un [modÃ¨le TinyBERT](https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D) spÃ©cifiquement pour notre projet. Le choix de TinyBERT Ã©tait motivÃ© par sa structure lÃ©gÃ¨re et efficace, rendant possible une haute performance tout en maintenant une consommation de ressources rÃ©duite. TinyBERT se distingue dans le domaine du TAL par sa capacitÃ© Ã  comprimer les modÃ¨les de langue de grande taille tout en prÃ©servant leur puissance et leur efficacitÃ©. 

## ğŸ¯ Objectif

Notre mission principale est de crÃ©er un portail web intergalactique qui permet aux utilisateurs terriens de soumettre du texte en franÃ§ais et d'obtenir des annotations de NER d'une prÃ©cision stellaire en utilisant quatre modÃ¨les distincts : le vÃ©nÃ©rable spaCy, l'illustre BERT-base (dslim/bert-base-NER), l'Ã©nigmatique Wikineural (Babelscape/wikineural-multilingual-ner) et l'intrÃ©pide TinyBERT, finetunÃ© avec le prÃ©cieux [MultiNERD dataset](https://huggingface.co/datasets/Babelscape/multinerd). L'objectif est d'offrir aux utilisateurs un moyen direct et visuellement intuitif de comparer les performances de chaque modÃ¨le, y compris notre adaptation personnalisÃ©e, pour rÃ©pondre Ã  des besoins spÃ©cifiques dans le domaine de la NER. Cette initiative entend proposer une ressource transparente et facilement accessible, destinÃ©e Ã  faciliter la recherche et l'exploitation pratique de la reconnaissance d'entitÃ©s nommÃ©es (NER).

## ğŸ‘©â€ğŸš€ğŸ‘¨â€ğŸš€ Auteurs

Cette aventure cosmique a Ã©tÃ© entreprise par Yingzi LIU et Xiaohua CUI, deux courageux Ã©tudiants de l'UniversitÃ© Sorbonne Nouvelle, armÃ©s de leur curiositÃ© et de leur soif de connaissance.

## âœ¨ FonctionnalitÃ©s

- **Multi-modÃ¨les de Reconnaissance d'EntitÃ©s NommÃ©es (NER)**: Emploi de quatre puissants modÃ¨les de NER pour classifier et annoter le texte soumis, rÃ©vÃ©lant les entitÃ©s telles que les personnes, lieux, organisations, etc.
- **Interface Web**: Nous proposons une interface web Ã©laborÃ©e et ergonomique, destinÃ©e aux utilisateurs dÃ©sirant analyser des textes de maniÃ¨re dÃ©taillÃ©e. Ce portail permet de soumettre aisÃ©ment des Ã©crits ou documents pour une exploration NER complÃ¨te.

## ğŸ“¦ Contenu du Projet

1. **ModÃ¨les de NER**: Une quÃªte Ã©pique Ã  travers diffÃ©rents modÃ¨les de NER, cherchant Ã  dÃ©terminer le plus vaillant.
2. **Fine-tuning de TinyBERT**: L'art dÃ©licat d'adapter un modÃ¨le TinyBERT avec le trÃ©sor cachÃ© qu'est le [MultiNERD dataset](https://huggingface.co/datasets/Babelscape/multinerd), dans le but d'amÃ©liorer sa performance sur des textes spÃ©cifiques.
3. **DÃ©veloppement d'une Interface Web**: La construction d'un portail interstellaire pour soumettre des textes et visualiser les rÃ©sultats de NER, un vÃ©ritable pont entre l'homme et la machine.

## Installation et exÃ©cution ğŸ› ï¸

1. Clonez le dÃ©pÃ´t GitHub sur votre navire spatial (machine locale).
2. Installez les composants nÃ©cessaires avec `pip install -r requirements.txt`.
3. Lancez le serveur web avec `python app.py`, ou via terminal avec un serveur Web comme `uvicorn`
4. AccÃ©dez Ã  l'interface web via votre navigateur spatial pour soumettre du texte ou des fichiers et recevoir des annotations de NER dignes d'une civilisation avancÃ©e.

>  Vous devez installer un serveur Web asynchrone tel que `uvicorn`, ainsi que les modules nÃ©cessaires tels que `fastapi`. 
>
> De plus, il pourrait Ãªtre nÃ©cessaire de tÃ©lÃ©charger diffÃ©rents modÃ¨les via `transformers`, nous vous recommandons donc fortement de tÃ©lÃ©charger ces Ã©lÃ©ments via une connexion Wi-Fi stable en raison de la taille potentielle des fichiers.

## ğŸ¤ Contributions

Nous encourageons les contributions de tous les coins de l'univers ! Pour signaler un bug interstellaire ou proposer des amÃ©liorations cosmiques, ouvrez une issue ou soumettez une pull request.

## ğŸ§ª AmÃ©liorations et expÃ©rimentations avec LLMs 

Notre projet est en constante Ã©volution, surtout Ã  l'Ã¨re des grands modÃ¨les de langage. Nous avons expÃ©rimentÃ© avec des LLMs plus petits, tels que `LLama2` et `Qwen1.5`. Notamment, `Qwen1.5`, un grand modÃ¨le de langage produit en Chine, offre un modÃ¨le de 0.5B, se caractÃ©risant par sa compacitÃ© et son efficacitÃ©. Cependant, en raison des besoins spÃ©cifiques de notre projet et des problÃ¨mes de compatibilitÃ© avec notre systÃ¨me de dÃ©veloppement sur Mac, nous avons dÃ» abandonner cette option.

N'hÃ©sitez pas Ã  contribuer et Ã  nous faire part de vos suggestions d'amÃ©lioration ! ğŸŒŸ

## ğŸ“ Contact

Pour toute question ou pour transmettre des signaux de dÃ©tresse, contactez-nous Ã  :

- Yingzi LIU : yingzi.liu@sorbonne-nouvelle.fr
- Xiaohua CUI : xiaohua.cui@sorbonne-nouvelle.fr

Nous vous remercions infiniment pour votre intÃ©rÃªt dans notre projet et nous espÃ©rons que vous y trouverez autant de plaisir et d'Ã©merveillement que nous en avons eu Ã  le dÃ©velopper !

## ğŸ“œ Licence

Ce projet est distribuÃ© sous la licence MIT, un pacte de bonne volontÃ© intergalactique. Pour plus de dÃ©tails, consultez le fichier LICENSE.



