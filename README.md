# üöÄ Reconnaissance d'Entit√©s Nomm√©es avec Multiples Mod√®les et Interface Web üöÄ

Ce projet a √©t√© d√©velopp√© avec passion et d√©vouement dans le cadre des cours "Interfaces web pour le TAL" et "R√©seaux de neurones", brillamment dirig√©s par M. Lo√Øc Grobol √† l'Universit√© Paris Nanterre.

## üìö Introduction

Dans le cadre de notre exploration approfondie de l'univers de la reconnaissance d'entit√©s nomm√©es (NER), nous avons mis en ≈ìuvre une approche m√©thodique pour √©tudier et comparer l'efficacit√© de divers mod√®les de renom dans ce domaine. Notre qu√™te nous a men√©s √† s√©lectionner et √† utiliser trois mod√®les pr√©-entra√Æn√©s particuli√®rement distingu√©s, chacun apportant une contribution unique √† notre compr√©hension globale de la NER. Voici les mod√®les que nous avons choisis pour notre voyage :

- **spaCy** : Reconnu pour sa rapidit√© et son efficacit√©, le mod√®le fr_core_news_sm de spaCy offre une base solide pour la reconnaissance d'entit√©s en fran√ßais. C'est un choix privil√©gi√© pour les applications n√©cessitant une grande vitesse de traitement et une bonne pr√©cision.

- **dslim/bert-base-NER** : Ce mod√®le, bas√© sur l'architecture Transformer de BERT et pr√©-entra√Æn√© sur un vaste corpus, est sp√©cialis√© dans la reconnaissance d'entit√©s nomm√©es. Il est connu pour sa capacit√© √† comprendre le contexte des mots dans des phrases, ce qui le rend extr√™mement efficace pour identifier avec pr√©cision divers types d'entit√©s.

- **Babelscape/wikineural-multilingual-ner** : Ce mod√®le tire parti de l'apprentissage profond pour offrir une reconnaissance d'entit√©s nomm√©es dans plusieurs langues, y compris le fran√ßais. Gr√¢ce √† son entra√Ænement sur des donn√©es provenant de Wikipedia, il est capable de reconna√Ætre un large √©ventail d'entit√©s √† travers diff√©rents domaines.

- **Fine-tuned TinyBERT** : Au-del√† de l'adoption de mod√®les pr√©-entra√Æn√©s r√©put√©s, nous avons √©galement pris l'initiative ambitieuse d'adapter un mod√®le pour r√©pondre pr√©cis√©ment √† nos exigences particuli√®res. En tirant parti du [MultiNERD dataset](https://huggingface.co/datasets/Babelscape/multinerd) , Nous avons proc√©d√© au fine-tuning d'un [mod√®le TinyBERT](https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D) sp√©cifiquement pour notre projet. Le choix de TinyBERT √©tait motiv√© par sa structure l√©g√®re et efficace, rendant possible une haute performance tout en maintenant une consommation de ressources r√©duite. TinyBERT se distingue dans le domaine du TAL par sa capacit√© √† comprimer les mod√®les de langue de grande taille tout en pr√©servant leur puissance et leur efficacit√©. 

## üéØ Objectif

Notre mission principale est de cr√©er un portail web intergalactique qui permet aux utilisateurs terriens de soumettre du texte en fran√ßais et d'obtenir des annotations de NER d'une pr√©cision stellaire en utilisant quatre mod√®les distincts : le v√©n√©rable spaCy, l'illustre BERT-base (dslim/bert-base-NER), l'√©nigmatique Wikineural (Babelscape/wikineural-multilingual-ner) et l'intr√©pide TinyBERT, finetun√© avec le pr√©cieux [MultiNERD dataset](https://huggingface.co/datasets/Babelscape/multinerd). L'objectif est d'offrir aux utilisateurs un moyen direct et visuellement intuitif de comparer les performances de chaque mod√®le, y compris notre adaptation personnalis√©e, pour r√©pondre √† des besoins sp√©cifiques dans le domaine de la NER. Cette initiative entend proposer une ressource transparente et facilement accessible, destin√©e √† faciliter la recherche et l'exploitation pratique de la reconnaissance d'entit√©s nomm√©es (NER).

## üë©‚ÄçüöÄüë®‚ÄçüöÄ Auteurs

Cette aventure cosmique a √©t√© entreprise par Yingzi LIU et Xiaohua CUI, deux courageux √©tudiants de l'Universit√© Sorbonne Nouvelle, arm√©s de leur curiosit√© et de leur soif de connaissance.

## ‚ú® Fonctionnalit√©s

- **Multi-mod√®les de Reconnaissance d'Entit√©s Nomm√©es (NER)**: Emploi de quatre puissants mod√®les de NER pour classifier et annoter le texte soumis, r√©v√©lant les entit√©s telles que les personnes, lieux, organisations, etc.
- **Interface Web**: Nous proposons une interface web √©labor√©e et ergonomique, destin√©e aux utilisateurs d√©sirant analyser des textes de mani√®re d√©taill√©e. Ce portail permet de soumettre ais√©ment des √©crits ou documents pour une exploration NER compl√®te.

## üì¶ Contenu du Projet

1. **Mod√®les de NER**: Une qu√™te √©pique √† travers diff√©rents mod√®les de NER, cherchant √† d√©terminer le plus vaillant.
2. **Fine-tuning de TinyBERT**: L'art d√©licat d'adapter un mod√®le TinyBERT avec le tr√©sor cach√© qu'est le [MultiNERD dataset](https://huggingface.co/datasets/Babelscape/multinerd), dans le but d'am√©liorer sa performance sur des textes sp√©cifiques.
3. **D√©veloppement d'une Interface Web**: La construction d'un portail interstellaire pour soumettre des textes et visualiser les r√©sultats de NER, un v√©ritable pont entre l'homme et la machine.

## üöÄ Utilisation

1. Clonez le d√©p√¥t GitHub sur votre navire spatial (machine locale).
2. Installez les composants n√©cessaires avec `pip install -r requirements.txt`.
3. Lancez le serveur web avec `python app.py`.
4. Acc√©dez √† l'interface web via votre navigateur spatial pour soumettre du texte ou des fichiers et recevoir des annotations de NER dignes d'une civilisation avanc√©e.

## ü§ù Contributions

Nous encourageons les contributions de tous les coins de l'univers ! Pour signaler un bug interstellaire ou proposer des am√©liorations cosmiques, ouvrez une issue ou soumettez une pull request.

## üìú Licence

Ce projet est distribu√© sous la licence MIT, un pacte de bonne volont√© intergalactique. Pour plus de d√©tails, consultez le fichier LICENSE.

## üìû Contact

Pour toute question ou pour transmettre des signaux de d√©tresse, contactez-nous √† :

- Yingzi LIU : yingzi.liu@sorbonne-nouvelle.fr
- Xiaohua CUI : xiaohua.cui@sorbonne-nouvelle.fr

Nous vous remercions infiniment pour votre int√©r√™t dans notre projet et nous esp√©rons que vous y trouverez autant de plaisir et d'√©merveillement que nous en avons eu √† le d√©velopper !
